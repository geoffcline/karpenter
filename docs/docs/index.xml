<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Karpenter – Documentation</title>
    <link>karpenter/docs/</link>
    <description>Recent content in Documentation on Karpenter</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jan 2017 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="karpenter/docs/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: FAQs</title>
      <link>karpenter/docs/faqs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>karpenter/docs/faqs/</guid>
      <description>
        
        
        &lt;h2 id=&#34;general&#34;&gt;General&lt;/h2&gt;
&lt;h3 id=&#34;how-does-a-provisioner-decide-to-manage-a-particular-node&#34;&gt;How does a Provisioner decide to manage a particular node?&lt;/h3&gt;
&lt;p&gt;&amp;laquo;&amp;laquo;&amp;laquo;&amp;lt; HEAD:website/content/en/docs/faq.md
Each node will have a set of predetermined Karpenter labels. Provisioners will use the &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;namespace&lt;/code&gt; labels to distinguish between Provisioners. Furthermore, a Provisioner will only take action on a node based on the label that details what phase a node is in, e.g. a Provisioner will only consider a node for termination if its phase label says &lt;code&gt;&amp;quot;underutilized&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;provisioning&#34;&gt;Provisioning&lt;/h2&gt;
&lt;p&gt;=======
Each node will have a set of predetermined Karpenter labels. Provisioners will use a &lt;code&gt;name&lt;/code&gt; label to distinguish between Provisioners. Furthermore, a Provisioner will only take action on a node based on the label that details what phase a node is in. e.g. a provisioner will only consider a node for termination if its phase label says &amp;ldquo;underutilized&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;allocation&#34;&gt;Allocation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;eaa48cf (update filenames and get started guide):website/content/en/docs/faqs.md&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;how-should-i-define-scheduling-constraints&#34;&gt;How should I define scheduling constraints?&lt;/h3&gt;
&lt;p&gt;Karpenter takes a layered approach to scheduling constraints. Each Cloud Provider has its own set of global defaults, which are overriden by defaults specified in the Provisioner, which are overridden by Pod scheduling constraints. This model requires minimal configuration for most use cases, and supports diverse workloads using a single Provisioner.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-replace-the-kube-scheduler&#34;&gt;Does Karpenter replace the Kube Scheduler?&lt;/h3&gt;
&lt;p&gt;No. Provisioners work in tandem with the Kube Scheduler. When capacity is unconstrained, the Kube Scheduler will schedule pods as usual. It may schedule pods to nodes managed by Provisioners or other types of capacity in the cluster. Provisioners only attempt to schedule pods when &lt;code&gt;type=PodScheduled,reason=Unschedulable&lt;/code&gt;. In this case, they will make a provisioning decision, launch new capacity, and bind pods to the provisioned nodes. Provisioners do not wait for the Kube Scheduler to make a scheduling decision in this case, as the decision is already made by nature of making a provisioning decision. It&amp;rsquo;s possible that a node from another management solution, like the Cluster Autoscaler, could create a race between the &lt;code&gt;kube-scheduler&lt;/code&gt; and Karpenter. In this case, the first binding call will win, although Karpenter will often win these race conditions due to its performance characteristics. If Karpenter loses this race, the node will eventually be cleaned up.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-node-selectors&#34;&gt;Does Karpenter support node selectors?&lt;/h3&gt;
&lt;p&gt;Yes. Node selectors are an opt-in mechanism which allow customers to specify the nodes on which a pod can scheduled. Provisioners recognize well-known node selectors on incoming pods and use them to constrain the nodes they generate. You can read more about the well-known node selectors Karpenter supports in the &lt;a href=&#34;karpenter/docs/concepts/#well-known-labels&#34;&gt;Concepts&lt;/a&gt; documentation. For example, well known selectors like &lt;code&gt;node.kubernetes.io/instance-type&lt;/code&gt;, &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;, &lt;code&gt;kubernetes.io/os&lt;/code&gt;, &lt;code&gt;kubernetes.io/arch&lt;/code&gt; are supported, and will ensure that provisioned nodes are constrained accordingly. Additionally, customers may specify arbitrary labels, which will be automatically applied to every node launched by the Provisioner.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-taints&#34;&gt;Does Karpenter support taints?&lt;/h3&gt;
&lt;p&gt;Yes. Taints are an opt-out mechanism which allows customers to specify the nodes on which a pod cannot be scheduled. Unlike labels, Provisioners do not automatically taint nodes in response to pod tolerations, since pod tolerations do not require that corresponding taints exist. However, similar to labels, customers may specify taints for their Provisioner, which will automatically be applied to every node in the group. This means that if a Provisioner is configured with taints, any incoming pods will not be provisioned unless they tolerate the taints.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-topology-spread-constraints&#34;&gt;Does Karpenter support topology spread constraints?&lt;/h3&gt;
&lt;p&gt;Yes. Provisioners respect &lt;code&gt;pod.spec.topologySpreadConstraints&lt;/code&gt;. Allocating pods with these constraints may yield highly fragmented nodes, due to their strict nature and complexity of “online binpacking” algorithms.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-affinity&#34;&gt;Does Karpenter support affinity?&lt;/h3&gt;
&lt;p&gt;No. Karpenter intentionally does not support affinity due the to &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity&#34;&gt;scalability limitations&lt;/a&gt; outlined by SIG Scalability. Instead, we recommend using node selectors or taints instead of node affinity and pod topology spread instead of pod affinity. Do you have a use case for affinity that we&amp;rsquo;re missing? Open an issue in our &lt;a href=&#34;https://github.com/awslabs/karpenter/issues/new/choose&#34;&gt;GitHub repo&lt;/a&gt; and tell us about it!&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-custom-resource-like-accelerators-or-hpc&#34;&gt;Does Karpenter support custom resource like accelerators or HPC?&lt;/h3&gt;
&lt;p&gt;Yes. Support for specific custom resources can be implemented by your cloud provider.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-daemonsets&#34;&gt;Does Karpenter support daemonsets?&lt;/h3&gt;
&lt;p&gt;Yes. Provisioners factor in daemonset overhead into all allocation calculations. They also respect daemonset scheduling constraints, such as Nvidia’s GPU Driver Installer.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-multiple-scheduling-defaults&#34;&gt;Does Karpenter support multiple scheduling defaults?&lt;/h3&gt;
&lt;p&gt;Provisioners are heterogeneous, which means that the nodes they manage are spread across multiple availability zones, instance types, and capacity types. This flexibility reduces the need for a large number of groups. However, customers may find multiple groups to be useful for more advanced use cases. For example, customers can create multiple groups, and then use the node selector &lt;code&gt;karpenter.sh/provisioner-name&lt;/code&gt; to target specific groups. This enables advanced use cases like resource isolation and sharding.&lt;/p&gt;
&lt;h3 id=&#34;what-if-my-pod-is-schedulable-for-multiple-provisioners&#34;&gt;What if my pod is schedulable for multiple Provisioners?&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s possible that unconstrained pods could flexibly schedule in multiple groups. In this case, Provisioners will race to create a scheduling lease for the pod before launching new nodes, which avoids unnecessary scale out.&lt;/p&gt;
&lt;h2 id=&#34;deprovisioning&#34;&gt;Deprovisioning&lt;/h2&gt;
&lt;h3 id=&#34;how-does-karpenter-decide-which-nodes-it-can-terminate&#34;&gt;How does Karpenter decide which nodes it can terminate?&lt;/h3&gt;
&lt;p&gt;A provisioner will only take action on nodes that it manages. This means that a node will only be considered for termination if it is labeled underutilized by the provisioner that manages it.&lt;/p&gt;
&lt;h3 id=&#34;how-do-i-know-if-a-node-is-underutilized&#34;&gt;How do I know if a node is underutilized?&lt;/h3&gt;
&lt;p&gt;Nodes are labeled underutilized if they have 0 non-daemonset pods scheduled. We plan to include more use cases in the future. A node needs to be underutilized for a period of time before being considered for termination.&lt;/p&gt;
&lt;h3 id=&#34;how-does-karpenter-terminate-nodes&#34;&gt;How does Karpenter terminate nodes?&lt;/h3&gt;
&lt;p&gt;Karpenter annotates nodes that are underutilized with a time to live (TTL). If the node remains underutilized after the TTL expires, Karpenter then &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration&#34;&gt;cordons&lt;/a&gt; the node and uses the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#eviction-api&#34;&gt;Kubernetes Eviction API&lt;/a&gt; to evict all non-daemonset pods. Once the node is empty, the node is terminated.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-pod-disruption-budgets&#34;&gt;Does Karpenter support Pod Disruption Budgets?&lt;/h3&gt;
&lt;p&gt;Yes. The Kubernetes Eviction API will not delete pods that violate a &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&#34;&gt;Pod Disruption Budget (PDB)&lt;/a&gt;. It also disallows eviction of any pod covered by multiple PDBs, so most users will want to avoid overlapping selectors. See &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets&#34;&gt;this&lt;/a&gt; for more.&lt;/p&gt;
&lt;h3 id=&#34;does-karpenter-support-scale-to-zero&#34;&gt;Does Karpenter support scale to zero?&lt;/h3&gt;
&lt;p&gt;Yes. Provisioners start at zero and launch or terminate nodes as necessary. We recommend that customers maintain a small amount of static capacity to bootstrap system controllers or run Karpenter outside of their cluster.&lt;/p&gt;
&lt;h2 id=&#34;compatibility&#34;&gt;Compatibility&lt;/h2&gt;
&lt;h3 id=&#34;which-kubernetes-versions-does-karpenter-support&#34;&gt;Which Kubernetes versions does Karpenter support?&lt;/h3&gt;
&lt;p&gt;Karpenter releases on a similar cadence to upstream Kubernetes releases. Currently, Karpenter is compatible with all Kubernetes versions greater than v1.16. However, this may change in the future as Karpenter takes dependencies on new Kubernetes features.&lt;/p&gt;
&lt;h3 id=&#34;can-i-use-karpenter-alongside-another-node-management-solution&#34;&gt;Can I use Karpenter alongside another node management solution?&lt;/h3&gt;
&lt;p&gt;Provisioners are designed to work alongside static capacity management solutions like EKS Managed Node Groups and EC2 Auto Scaling Groups. Some customers may choose to (1) manage the entirety of their capacity using Provisioner, others may prefer (2) a mixed model with both dynamic and statically managed capacity, some may prefer (3) a fully static approach. We anticipate that most customers will fall into bucket (2) in the short term, and (1) in the long term.&lt;/p&gt;
&lt;h3 id=&#34;can-i-use-karpenter-with-the-kubernetes-cluster-autoscaler&#34;&gt;Can I use Karpenter with the Kubernetes Cluster Autoscaler?&lt;/h3&gt;
&lt;p&gt;Yes, with side effects. Karpenter is a Cluster Autoscaler replacement. Both systems scale up nodes in response to unschedulable pods. If configured together, both systems will race to launch new instances for these pods. Since Provisioners make binding decisions, Karpenter will typically win the scheduling race. In this case, the Cluster Autoscaler will eventually scale down the unnecessary capacity. If the Cluster Autoscaler is configured with Node Groups that have constraints that aren’t supported by any Provisioner, its behavior will continue unimpeded.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Provisioner CRD</title>
      <link>karpenter/docs/provisioner-crd/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>karpenter/docs/provisioner-crd/</guid>
      <description>
        
        
        &lt;h2 id=&#34;example-resource&#34;&gt;example resource&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;apiVersion: karpenter.sh/v1alpha3
kind: Provisioner
metadata:
  name: default
spec:
  &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Provisioned nodes connect to this cluster&lt;/span&gt;
  cluster:
    name: &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;CLUSTER_NAME&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;&lt;/span&gt;
    caBundle: &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;CLUSTER_CA_BUNDLE&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;&lt;/span&gt;
    endpoint: &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;CLUSTER_ENDPOINT&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;&lt;/span&gt;

  &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# If nil, the feature is disabled, nodes will never expire&lt;/span&gt;
  ttlSecondsUntilExpired: &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;2592000&lt;/span&gt; &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# 30 Days = 60 * 60 * 24 * 30 Seconds;&lt;/span&gt;

  &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# If nil, the feature is disabled, nodes will never scale down due to low utilization&lt;/span&gt;
  ttlSecondsAfterEmpty: &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;30&lt;/span&gt;

  &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Provisioned nodes will have these taints&lt;/span&gt;
  taints:
    - key: example.com/special-taint
      effect: NoSchedule

  &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Provisioned nodes will have these labels&lt;/span&gt;
  labels:
    &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;##### AWS Specific #####&lt;/span&gt;
    &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Constrain node launch template, default=&amp;#34;bottlerocket&amp;#34;&lt;/span&gt;
    node.k8s.aws/launch-template-id: &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;bottlerocket-qwertyuiop&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Constrain node launch template, default=&amp;#34;$LATEST&amp;#34;&lt;/span&gt;
    node.k8s.aws/launch-template-version: &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;my-special-version&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# Constrain node capacity type, default=&amp;#34;on-demand&amp;#34;&lt;/span&gt;
    node.k8s.aws/capacity-type: &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;spot&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Development Guide</title>
      <link>karpenter/docs/development-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>karpenter/docs/development-guide/</guid>
      <description>
        
        
        &lt;h2 id=&#34;dependencies&#34;&gt;Dependencies&lt;/h2&gt;
&lt;p&gt;The following tools are required for contributing to the Karpenter project.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Package&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Install&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://golang.org/dl/&#34;&gt;go&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.15.3+&lt;/td&gt;
&lt;td&gt;&lt;code&gt;brew install go&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;kubectl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;brew install kubectl&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://helm.sh/docs/intro/install/&#34;&gt;helm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;brew install helm&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Other tools&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;make toolchain&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;developing&#34;&gt;Developing&lt;/h2&gt;
&lt;h3 id=&#34;setup--teardown&#34;&gt;Setup / Teardown&lt;/h3&gt;
&lt;p&gt;Based on how you are running your Kubernetes cluster, follow the &lt;a href=&#34;#environment-specific-setup&#34;&gt;Environment specific setup&lt;/a&gt; to configure your environment before you continue. Once you have your environment set up, to install Karpenter in the Kubernetes cluster specified in your &lt;code&gt;~/.kube/config&lt;/code&gt;  run the following commands.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make codegen # Create auto-generated YAML files.
make apply # Install Karpenter
make delete # Uninstall Karpenter
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;developer-loop&#34;&gt;Developer Loop&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Make sure dependencies are installed
&lt;ul&gt;
&lt;li&gt;Run &lt;code&gt;make codegen&lt;/code&gt; to make sure yaml manifests are generated&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;make toolchain&lt;/code&gt; to install cli tools for building and testing the project&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You will need a personal development image repository (e.g. ECR)
&lt;ul&gt;
&lt;li&gt;Make sure you have valid credentials to your development repository.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$KO_DOCKER_REPO&lt;/code&gt; must point to your development repository&lt;/li&gt;
&lt;li&gt;Your cluster must have permissions to read from the repository&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If you created your cluster on version 1.19 or above, you may need to tag your subnets as mentioned &lt;a href=&#34;docs/aws/README.md&#34;&gt;here&lt;/a&gt;. This is a temporary problem with our subnet discovery system, and is being tracked &lt;a href=&#34;https://github.com/awslabs/karpenter/issues/404#issuecomment-845283904&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;build-and-deploy&#34;&gt;Build and Deploy&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;make dev                                  # build and test code
make apply                                # deploy local changes to cluster
CLOUD_PROVIDER=&amp;lt;YOUR_PROVIDER&amp;gt; make apply # deploy for your cloud provider
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;testing&#34;&gt;Testing&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;make test       # E2e correctness tests
make battletest # More rigorous tests run in CI environment
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;verbose-logging&#34;&gt;Verbose Logging&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl patch deployment karpenter -n karpenter --type&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;json&amp;#39;&lt;/span&gt; -p&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;replace&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/containers/0/args&amp;#34;, &amp;#34;value&amp;#34;: [&amp;#34;--verbose&amp;#34;]}]&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;debugging-metrics&#34;&gt;Debugging Metrics&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;open http://localhost:8080/metrics &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; kubectl port-forward service/karpenter-metrics -n karpenter &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;8080&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;environment-specific-setup&#34;&gt;Environment specific setup&lt;/h2&gt;
&lt;h3 id=&#34;aws&#34;&gt;AWS&lt;/h3&gt;
&lt;p&gt;Set the CLOUD_PROVIDER environment variable to build cloud provider specific packages of Karpenter.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export CLOUD_PROVIDER=aws
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For local development on Karpenter you will need a Docker repo which can manage your images for Karpenter components.
You can use the following command to provision an ECR repository.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws ecr create-repository \
    --repository-name karpenter/controller \
    --image-scanning-configuration scanOnPush=true \
    --region ${AWS_DEFAULT_REGION}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once you have your ECR repository provisioned, configure your Docker daemon to authenticate with your newly created repository.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export KO_DOCKER_REPO=&amp;quot;${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/karpenter&amp;quot;
aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin $KO_DOCKER_REPO
&lt;/code&gt;&lt;/pre&gt;
      </description>
    </item>
    
  </channel>
</rss>
